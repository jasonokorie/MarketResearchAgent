{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGTEEQ-H8w-e"
      },
      "source": [
        "# Workshop: Building a Market Research Agent\n",
        "\n",
        "Welcome! In this notebook, we'll build a smart AI Agent that can:\n",
        "\n",
        "1.  **Chat with a user** to understand their market of their startup.\n",
        "2.  **Decide** when it has gathered enough information to move to research.\n",
        "3.  **Use external tools** (like Perplexity's API) to narrow down market research niches.\n",
        "4.  **Analyze the findings** and the conversation.\n",
        "5.  **Generate a structured report.**\n",
        "\n",
        "\n",
        "* **LLMs as Orchestrators:** We'll use Large Language Models (LLMs) like Google's Gemini can evolve beyond only generating text and also *control a workflow*, make decisions, and use tools.\n",
        "* **LangChain & LangGraph:** We'll use these powerful libraries designed to make building complex AI applications easier. LangGraph helps create reliable, step-by-step AI processes through state management.\n",
        "* **Real-world Pattern:** Similar patterns to this one (chat -> gather info -> use tools -> synthesize) are common in many professions, and can be leveraged to automate real jobs.\n",
        "\n",
        "**Prerequisites:** Gemini and Perplexity API. We'll explain the AI concepts as we go!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkOyUc3xpDSM"
      },
      "source": [
        "## Before we start, you need 2 API keys:\n",
        "1. Get free Gemini API key at https://ai.google.dev/gemini-api/docs/api-key\n",
        "2. Get free Perplexity API Key (by using your school email) at https://www.perplexity.ai/referrals/join\n",
        "\n",
        "(You would need to use credit card information to access the Perplexity API once you create an account. You get 5$ of free API credits per month for Perplexity from your student email account and Gemini API is free. So it will not cost anything.)\n",
        "\n",
        "3. Make a copy of this colab notebook, and open your copy. Select the secrets option on left-side handle, and add the GEMINI_API_KEY and PERPLEXITY_API_KEY fields, pasting the API keys in the text fields. This saves the API keys to colab and make them accessible to the colab notebook we'll use for our workshop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mekvUtk7BBtV"
      },
      "source": [
        "## Step 1: Import libraries we'll be using in the workshop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "On5Lt4-0dfy-",
        "outputId": "5c367ee4-3405-45d1-f207-8023c70b4327"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.16 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.52 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.56)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.3)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.29.4)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.3.34)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (4.13.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.52->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.3-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m502.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.17 langchain-google-genai-2.1.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "f1157195c0cb4f428680222e0f0cac5a",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.3.34-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.56)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.25-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.8 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.63-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (0.3.34)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (2.11.3)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
            "  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.16)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.3.34-py3-none-any.whl (148 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.25-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.63-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.3.34 langgraph-checkpoint-2.0.25 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.63 ormsgpack-1.9.1 xxhash-3.5.0\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.3.34)\n",
            "Collecting langsmith\n",
            "  Downloading langsmith-0.3.38-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.56)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.10.16)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (4.13.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx) (1.3.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
            "Downloading langsmith-0.3.38-py3-none-any.whl (359 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.3/359.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langsmith\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.34\n",
            "    Uninstalling langsmith-0.3.34:\n",
            "      Successfully uninstalled langsmith-0.3.34\n",
            "Successfully installed langsmith-0.3.38\n"
          ]
        }
      ],
      "source": [
        "# ---- Core Python & Utilities ---- #\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time # used for rate-limiting\n",
        "import requests # for making web requests to Perplexity API\n",
        "from functools import wraps # Helper used for building decorators easily\n",
        "from typing import List, Dict, Any # For type hinting\n",
        "from typing_extensions import Annotated, TypedDict # For advanced type for our state\n",
        "from google.colab import userdata # a secure way to access keys in colab we've saved as secrets\n",
        "from rich.console import Console\n",
        "from rich.markdown import Markdown\n",
        "\n",
        "# ---- Langchain, LangGraph & Google API ---- #\n",
        "from operator import itemgetter\n",
        "import google.api_core.exceptions # for handling google API errors\n",
        "from google.generativeai import configure, list_models #\n",
        "!pip install langchain-google-genai # installs the LangChain Integration for google's models quietly\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "!pip install langgraph # installs the LangGraph library quietly\n",
        "from langgraph.graph import StateGraph, START, END # Core components for building the graph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "!pip install -U langchain langsmith httpx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnoJP4B4BLCo"
      },
      "source": [
        "## Step 2: Set up the APIs to use LLMs for the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2Dqw2rWBIJ8"
      },
      "outputs": [],
      "source": [
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "PERPLEXITY_API_KEY = userdata.get('PERPLEXITY_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA8FD6E3BkFT"
      },
      "source": [
        "## Extra step: Set up a function for API rate-limiting\n",
        "\n",
        "The functions are delayed for a few seconds every time it runs an API. Without strict controls, repeated or recursive API calls can quickly go out of control, leading to infinite loops or too many API requests in a few seconds. The result ? High billing cost, service denial by API provider, or even temporary bans.\n",
        "\n",
        "We will also be serving Perplexity API as a tool for Gemini LLM to invoke, so there's a chance of infinite loops in case Gemini decides to overdo the research."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TN7ctE80BkXz"
      },
      "outputs": [],
      "source": [
        "def api_rate_limit(seconds: int = 2): # default pause is 2 seconds\n",
        "    \"\"\"This nested function creates and returns a Decorator to add sleep time between API calls\"\"\"\n",
        "    def decorator(func):\n",
        "        @wraps(func) # Saves the metadata of wrapped function (like name, docstring) to\n",
        "        def wrapper(*args, **kwargs):\n",
        "            \"\"\"This wrapper executes the following code before the target function executes\"\"\"\n",
        "            time.sleep(seconds)  # Pause for some seconds before making the API call\n",
        "            return func(*args, **kwargs) # Now calls the original target function\n",
        "        return wrapper\n",
        "    return decorator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txL2sCsIDCMg"
      },
      "source": [
        "## Step 3: Defining the perplexity_research function as a tool for LLM to invoke\n",
        "\n",
        "Turning Perplexity API into a tool that Learn-lm-1.5-pro can use to deepen its analysis. The research tool is one of the most crucial steps as it turns a general LLM into an expert on any topic, giving it tools to research the web in real-time and augment its knowledge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eMCkaYNDCe8"
      },
      "outputs": [],
      "source": [
        "@tool # LangChain decorator. Now the function for Perplexity API is available as a tool for the LLM\n",
        "@api_rate_limit(1) # Apply our custom 1-second rate limit before calling this function each time\n",
        "def perplexity_research(query: str) -> str:\n",
        "    \"\"\"Research market value trends. This should include demographics data, growth trends, market size (TAM, SAM, and SOM). Provice citations and links to reliable, authentic market research sources and data.\"\"\"\n",
        "    headers = { # Standard HTTP headers for the API request\n",
        "        \"accept\": \"application/json\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {PERPLEXITY_API_KEY}\"\n",
        "    }\n",
        "    payload = { # The actual data sent to the Perplexity API\n",
        "        \"model\": \"sonar-pro\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\",\n",
        "            \"content\": \"You are a market research assistant. Provide precise and well-sourced responses, along with citations, and links for resources\"},\n",
        "            {\"role\": \"user\", \"content\": query}\n",
        "        ],\n",
        "        \"temperature\": 0.3,  # Lower randomness for factual consistency\n",
        "        \"max_tokens\": 2048,  # Allow more detailed responses\n",
        "        \"top_p\": 0.8,  # Nucleus sampling for high-confidence outputs\n",
        "        \"frequency_penalty\": 0.0,  # Reduce repetitive phrasing\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        print(\"RESPONSE: Sending request to Perplexity API...\")\n",
        "        response = requests.post(\"https://api.perplexity.ai/chat/completions\", json=payload, headers=headers)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Debugging API response\n",
        "        json_response = response.json()\n",
        "        print(f\"RESPONSE: API Response JSON: {json_response}\")\n",
        "\n",
        "        # Adjust parsing based on actual response structure\n",
        "        return json_response[\"choices\"][0].get(\"message\", {}).get(\"content\", \"No content found.\")\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"RESPONSE: API Error Details: {str(e)}\")\n",
        "        return f\"Error researching topic: {str(e)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGFO53x5DSOQ"
      },
      "source": [
        "## Step 4: Set up the LLM for interaction with user input and orchestration with the conversation flow\n",
        "\n",
        "Now that the tool for LLM has been built, let's bind it to the LLM and set up the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91eNyqkWDSi0"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"learnlm-1.5-pro-experimental\",  # You can try other models by replacing with \"gemini-1.5-pro\", \"gemini-1.5-flash\", or \"gemini-2.5-pro-preview-03-25\"\n",
        "    google_api_key=GEMINI_API_KEY,\n",
        "    temperature=0.3 # Range is usually 0 to 1, we are choosing lower value for more predictable responses\n",
        ")\n",
        "# Define a simple prompt template to turn questions into prompt for LLM\n",
        "template = \"Answer this to the best of your knowledge. {question} ?\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "# Bind tools with LLM using LangChain\n",
        "tools = [perplexity_research]\n",
        "llm_with_tools = llm.bind_tools(tools=tools)\n",
        "\n",
        "# State Management\n",
        "# Defines the structure for managing conversation state and analysis progress\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[List[Dict[str, Any]], \"Chat messages\"]                            # Store a list of all chat messages\n",
        "    research_results: Annotated[Dict[str, Any], \"Market research data\"]                  # Research results from Perplexity API\n",
        "    analysis_complete: Annotated[bool, \"Whether analysis is complete\"]                    # Determine if analysis is completed, in True or False\n",
        "    report: Annotated[Dict[str, Any], \"Final Market analysis report\"]                    # Final report to be returned\n",
        "    conversation_stage: Annotated[str, \"Current stage: conversation, research, complete\"] # Track the current stage for LLM\n",
        "    symptom_details: Annotated[Dict[str, Any], \"Collected symptom information\"]           # Details of symptoms, to be used by LLM\n",
        "    question_count: Annotated[int, \"Number of questions asked so far\"]                    # Not necessary unless we want min/max number of questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XypHmNBfDiXu"
      },
      "source": [
        "## Step 5: Setup the Conversation Flow Nodes to enable deeper research, analysis and report generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEtTVwG5Di-7"
      },
      "outputs": [],
      "source": [
        "# Initial Conversation Handler\n",
        "# Processes user input and generates initial response using STRUCTURED OUTPUT\n",
        "@api_rate_limit(1)\n",
        "def interactive_conversation(state: State):\n",
        "    \"\"\"Handle multi-turn conversation using structured JSON output from LLM\n",
        "       to dynamically decide when enough detail is present.\"\"\"\n",
        "    print(\"PROCESSING: Entering interactive_conversation node...\")\n",
        "    current_messages = state[\"messages\"]\n",
        "    question_count = state.get(\"question_count\", 0) + 1 # Still track for context/failsafe\n",
        "    symptom_details = state.get(\"symptom_details\", {})\n",
        "\n",
        "    # --- Failsafe Check (Optional but Recommended) ---\n",
        "    FAILSAFE_LIMIT = 10 # Set a max limit of questions to prevent potential infinite loops\n",
        "    if question_count > FAILSAFE_LIMIT:\n",
        "        print(f\"DEBUG ERROR: Failsafe question limit ({FAILSAFE_LIMIT}) reached. Forcing move to research.\")\n",
        "        # Update symptom details one last time\n",
        "        if current_messages and current_messages[-1].get(\"role\") == \"user\":\n",
        "            last_updated = symptom_details.get(\"last_updated\", -1)\n",
        "            if len(current_messages) > last_updated:\n",
        "                symptom_details = extract_symptom_details(current_messages)\n",
        "\n",
        "        # Construct a hardcoded message indicating the move to analysis due to limit.\n",
        "        response_content = \"Based on the information gathered so far, I will now proceed with the analysis.\"\n",
        "        new_message = {\"role\": \"assistant\", \"content\": response_content}\n",
        "        updated_messages = current_messages + [new_message]\n",
        "        return {\n",
        "            \"messages\": updated_messages,\n",
        "            \"question_count\": question_count -1, # Stay at the failsafe count\n",
        "            \"conversation_stage\": \"research\", # Force stage to research\n",
        "            \"symptom_details\": symptom_details\n",
        "        }\n",
        "    # --- End Failsafe Check ---\n",
        "\n",
        "    # Update symptom details if new user message arrived\n",
        "    if current_messages and current_messages[-1].get(\"role\") == \"user\":\n",
        "        last_updated = symptom_details.get(\"last_updated\", -1)\n",
        "        if len(current_messages) > last_updated:\n",
        "            print(\"PROCESSING: (interactive_conversation) Extracting details from latest user message...\")\n",
        "            symptom_details = extract_symptom_details(current_messages)\n",
        "\n",
        "\n",
        "    # --- Prompt requesting JSON output ---\n",
        "    prompt = f\"\"\"\n",
        "    {SYSTEM_PROMPT}\n",
        "\n",
        "    You are in the **information gathering** stage of a startup market analysis consultation. Your goal is to gather sufficient detail to perform a preliminary analysis following a standard procedure.\n",
        "    Conversation History:\n",
        "    {format_conversation_history(current_messages)} # Assuming this helper exists\n",
        "\n",
        "    Current Symptom Understanding (internal summary - may be incomplete):\n",
        "    {symptom_details.get(\"extracted_data\", \"No structured summary yet.\")}\n",
        "\n",
        "    Based on the conversation history and your understanding:\n",
        "\n",
        "    1.  **Assess Sufficiency:** Do you have enough detail about the main startup idea? Consider key aspects like the following, but remember ALL of these are not always necessary. Based on what the business opportunity could be, you decide only the RELEVANT pieces of info to ask:\n",
        "        * Core product or service description\n",
        "        * Target market specifics (demographics, needs)\n",
        "        * Pricing and revenue model\n",
        "        * Competitive landscape understanding\n",
        "        * Distribution channels\n",
        "        * Traction or validation milestones\n",
        "        * Market trends or timing factors\n",
        "\n",
        "    2.  **Decide Action and Format Output:** Respond ONLY with a valid JSON object containing two keys:\n",
        "        * `\"proceed_to_research\"`: A boolean value (`True` if you have sufficient detail based on the criteria, `False` otherwise).\n",
        "         * `\"assistant_message\"`: (string)\n",
        "              * If `True`, a brief, confident confirmation (e.g., \"Thank you. I have enough information to begin the market analysis.\").\n",
        "              * If `False`, the single, most important follow-up question needed right now. Keep it concise and professional (e.g., \"Could you tell me more about your target customer's key needs?\").\n",
        "\n",
        "    Example valid JSON output if continuing conversation:\n",
        "    {{\n",
        "      \"proceed_to_research\": False,\n",
        "      \"assistant_message\": \"Could you clarify how you plan to distribute your product to customers?\"\n",
        "    }}\n",
        "\n",
        "    Example valid JSON output if ready for research:\n",
        "    {{\n",
        "      \"proceed_to_research\": True,\n",
        "      \"assistant_message\": \"Thank you. I have enough information to begin the market analysis.\"\n",
        "    }}\n",
        "\n",
        "    This is conversation turn {question_count}. Ensure your entire response is ONLY the JSON object without any introductory text or explanation.\n",
        "    \"\"\"\n",
        "    print(f\"DEBUG: Invoking LLM for conversation (Turn {question_count}, assessing sufficiency, expecting JSON)...\")\n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        response_content = response.content if hasattr(response, 'content') else str(response)\n",
        "        print(f\"DEBUG: LLM raw response received: '{response_content[:100]}...'\") # Log more for debugging JSON\n",
        "\n",
        "        # --- Attempt to Parse JSON Response ---\n",
        "        try:\n",
        "            # Clean potential markdown code fences if the model wraps JSON in them\n",
        "            if response_content.strip().startswith(\"```json\"):\n",
        "                response_content = response_content.strip()[7:-3].strip()\n",
        "            elif response_content.strip().startswith(\"```\"):\n",
        "                 response_content = response_content.strip()[3:-3].strip()\n",
        "\n",
        "            parsed_data = json.loads(response_content)\n",
        "\n",
        "            # Validate expected keys and types (basic validation)\n",
        "            if not isinstance(parsed_data, dict) or \\\n",
        "               \"proceed_to_research\" not in parsed_data or \\\n",
        "               \"assistant_message\" not in parsed_data or \\\n",
        "               not isinstance(parsed_data[\"proceed_to_research\"], bool) or \\\n",
        "               not isinstance(parsed_data[\"assistant_message\"], str):\n",
        "                raise ValueError(\"Parsed JSON missing required keys or has incorrect types.\")\n",
        "\n",
        "            has_enough_info = parsed_data[\"proceed_to_research\"]\n",
        "            assistant_content = parsed_data[\"assistant_message\"]\n",
        "            print(f\"DEBUG: JSON parsed successfully. proceed_to_research={has_enough_info}\")\n",
        "\n",
        "        except (json.JSONDecodeError, ValueError) as json_error:\n",
        "            print(f\"ERROR: Failed to parse valid JSON or validate structure from LLM response: {json_error}\")\n",
        "            print(f\"LLM Raw Response causing error: {response_content}\")\n",
        "            has_enough_info = False # Default to continuing conversation on format error\n",
        "            assistant_content = \"I seem to be having trouble formatting my thoughts. Could you please clarify your last point or ask again?\"\n",
        "            # Optionally, you could use the raw response_content here if it might be readable\n",
        "\n",
        "    except Exception as llm_error:\n",
        "        print(f\"ERROR: LLM invocation failed in interactive_conversation: {llm_error}\")\n",
        "        has_enough_info = False # Default to continuing\n",
        "        assistant_content = \"I encountered an issue communicating. Could you please try again?\"\n",
        "        # No new_stage variable needed here as it's determined after the try-except block\n",
        "\n",
        "    # Determine the next stage based on the parsed boolean flag\n",
        "    new_stage = \"research\" if has_enough_info else \"conversation\"\n",
        "    print(f\"DEBUG: Based on parsed JSON/error handling: enough info? {has_enough_info}. New stage: {new_stage}\")\n",
        "\n",
        "    # Use the extracted message content\n",
        "    new_message = {\"role\": \"assistant\", \"content\": assistant_content}\n",
        "    updated_messages = current_messages + [new_message]\n",
        "\n",
        "    return {\n",
        "        \"messages\": updated_messages,\n",
        "        \"question_count\": question_count,\n",
        "        \"conversation_stage\": new_stage,\n",
        "        \"symptom_details\": symptom_details\n",
        "    }\n",
        "    # --- End Pre-check ---\n",
        "\n",
        "    # Update symptom details if new user message arrived since last extraction\n",
        "    if current_messages and current_messages[-1].get(\"role\") == \"user\":\n",
        "        last_updated = symptom_details.get(\"last_updated\", -1)\n",
        "        if len(current_messages) > last_updated:\n",
        "            symptom_details = extract_symptom_details(current_messages)\n",
        "\n",
        "    # Define prompt for the information gathering stage\n",
        "    prompt = f\"\"\"\n",
        "    {SYSTEM_PROMPT}\n",
        "\n",
        "    You are in the information gathering stage. This is question number {question_count}. Here's the conversation so far:\n",
        "    {format_conversation_history(current_messages)}\n",
        "\n",
        "    Based on this information, ask **ONE** specific, relevant follow-up question to gather more details about the startup concept already mentioned (such as target customer segments, pricing strategy, go-to-market plan, key differentiators, or competitive landscape).\n",
        "\n",
        "    Alternatively, if you assess that you have sufficient detail about the core startup concept (e.g., clear product description, target audience, and market intent provided), respond ONLY with the exact phrase: \"I have enough information to analyze your startup now.\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"DEBUG: Invoking LLM for conversation (Question {question_count})...\")\n",
        "    response = llm.invoke(prompt)\n",
        "    response_content = response.content if hasattr(response, 'content') else str(response)\n",
        "    print(f\"DEBUG: LLM response received: '{response_content[:100]}...'\")\n",
        "\n",
        "    # Check if the LLM decided it has enough information\n",
        "    has_enough_info = \"enough information\" in response_content.lower()\n",
        "\n",
        "    # Determine the next stage based ONLY on LLM response now (count check is done above)\n",
        "    new_stage = \"research\" if has_enough_info else \"conversation\"\n",
        "    print(f\"DEBUG: LLM indicated enough info? {has_enough_info}. New stage: {new_stage}\")\n",
        "\n",
        "    new_message = {\"role\": \"assistant\", \"content\": response_content}\n",
        "    updated_messages = current_messages + [new_message]\n",
        "\n",
        "    return {\n",
        "        \"messages\": updated_messages,\n",
        "        \"question_count\": question_count, # Pass the current count along\n",
        "        \"conversation_stage\": new_stage,\n",
        "        \"symptom_details\": symptom_details\n",
        "    }\n",
        "\n",
        "def format_conversation_history(messages):\n",
        "    \"\"\"Format the conversation history for the LLM prompt\"\"\"\n",
        "    formatted = \"\"\n",
        "    for msg in messages:\n",
        "        # Ensure content exists and is a string\n",
        "        content = msg.get(\"content\", \"\")\n",
        "        if not isinstance(content, str):\n",
        "             content = str(content) # Convert non-strings\n",
        "\n",
        "        role = \"User\" if msg.get(\"role\") == \"user\" else \"Assistant\"\n",
        "        formatted += f\"{role}: {content}\\n\\n\"\n",
        "    return formatted.strip() # Remove trailing newline\n",
        "\n",
        "@api_rate_limit(1) # Add rate limiting if desired\n",
        "def extract_symptom_details(messages):\n",
        "    \"\"\"Extract symptom information from user messages using LLM\"\"\"\n",
        "    # Combine relevant user messages\n",
        "    user_input_list = [\n",
        "        str(msg.get(\"content\", \"\")) # Ensure content is string\n",
        "        for msg in messages\n",
        "        if msg.get(\"role\") == \"user\"\n",
        "    ]\n",
        "    if not user_input_list:\n",
        "         return {\"extracted_data\": \"No user input found\", \"last_updated\": len(messages)}\n",
        "\n",
        "    all_user_input = \"\\n\".join(user_input_list)\n",
        "\n",
        "    extract_prompt = f\"\"\"\n",
        "    Based on the following user messages, extract and structure key symptom information:\n",
        "\n",
        "    {all_user_input}\n",
        "\n",
        "    Organize startup details into: Core product/service offering, Target customer profile (including demographics and needs), Market timing/trends, Competitive advantages or differentiators, Relevant business background or traction.\n",
        "    Return as concise, structured text (not strict JSON).\n",
        "\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"DEBUG: Extracting makrket details...\")\n",
        "        response = llm.invoke(extract_prompt)\n",
        "        extracted_content = response.content if hasattr(response, 'content') else str(response)\n",
        "        print(\"DEBUG: Market extraction complete.\")\n",
        "        return {\"extracted_data\": extracted_content, \"last_updated\": len(messages)}\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting market details: {str(e)}\")\n",
        "        # Provide error information but allow flow to continue\n",
        "        return {\"extracted_data\": f\"Error processing symptoms: {str(e)}\", \"last_updated\": len(messages)}\n",
        "\n",
        "# --- Placeholder for Waiting ---\n",
        "# This node doesn't do anything, it's just a named step in the graph\n",
        "# where the control flow pauses before the next user input in the command line loop.\n",
        "def wait_for_user_response(state: State):\n",
        "     \"\"\"Node indicating the graph is waiting for user input.\"\"\"\n",
        "     print(\"DEBUG: Entering wait_for_user_response node (waiting for input loop)...\")\n",
        "     # No state change needed here, just a logical pause point\n",
        "     return state\n",
        "\n",
        "# Research Determination\n",
        "# Analyzes symptoms and queries medical research\n",
        "# Modify the determine_research_needs function to explicitly use the tool:\n",
        "# --- Research Node ---\n",
        "# No rate-limit needed here as it calls perplexity_research, which already has rate-limiting\n",
        "def determine_research_needs(state: State):\n",
        "    \"\"\"Determine what conditions to research based on conversation.\"\"\"\n",
        "    print(\"DEBUG: Entering determine_research_needs node...\")\n",
        "    messages = state[\"messages\"]\n",
        "    symptom_details = state.get(\"symptom_details\", {})\n",
        "\n",
        "    # Use the structured details if available, otherwise fall back to user messages\n",
        "    extracted_data = symptom_details.get(\"extracted_data\", \"No structured data extracted.\")\n",
        "    if extracted_data == \"No structured data extracted.\" or \"Error processing symptoms\" in extracted_data:\n",
        "         # Fallback: use raw user input if extraction failed or didn't happen\n",
        "         user_input_list = [str(msg.get(\"content\",\"\")) for msg in messages if msg.get(\"role\") == \"user\"]\n",
        "         symptom_summary_for_research = \"\\n\".join(user_input_list)\n",
        "         print(\"DEBUG: Using raw user input for research prompt as structured data is unavailable/error.\")\n",
        "    else:\n",
        "        symptom_summary_for_research = extracted_data\n",
        "        print(\"DEBUG: Using extracted market details for research prompt.\")\n",
        "\n",
        "\n",
        "    research_prompt = f\"\"\"\n",
        "    Based on the following market information:\n",
        "    {symptom_summary_for_research}\n",
        "\n",
        "    Perform market research focusing on:\n",
        "    1. Most relevant markets and industry sectors (ranked by alignment with the startup’s concept).\n",
        "    2. Brief explanation of each sector’s relevance, size, growth trends, and key dynamics.\n",
        "    3. Cite relevant, authoritative sources (e.g., Statista, IBISWorld, PitchBook, McKinsey, BCG, Crunchbase; include links if possible).\n",
        "    4. Suggest potential next steps for market entry, validation, or expansion.\n",
        "    \"\"\"\n",
        "    print(\"RESPONSE: Starting Perplexity research...\")\n",
        "    # Ensure the tool gets a dictionary with 'query' key\n",
        "    results = perplexity_research.invoke({\"query\": research_prompt})\n",
        "    print(\"RESPONSE: Perplexity research complete.\")\n",
        "\n",
        "    # Store results correctly\n",
        "    return {\"research_results\": {\"medical_research\": results}} # Ensure results are nested if needed later\n",
        "\n",
        "# Processes research data and generates market analysis\n",
        "# --- Analysis Node ---\n",
        "@api_rate_limit(1)\n",
        "def generate_analysis(state: State):\n",
        "    \"\"\"Generate market analysis incorporating research.\"\"\"\n",
        "    print(\"DEBUG: Entering generate_analysis node...\")\n",
        "    # Correctly access nested research results\n",
        "    research_data = state.get('research_results', {}).get('medical_research', 'No research data available.')\n",
        "    messages = state[\"messages\"]\n",
        "    symptom_details = state.get(\"symptom_details\", {})\n",
        "\n",
        "    # Prepare symptom summary for analysis prompt\n",
        "    extracted_data = symptom_details.get(\"extracted_data\", \"No structured data.\")\n",
        "    if extracted_data == \"No structured data.\" or \"Error processing symptoms\" in extracted_data:\n",
        "         user_input_list = [str(msg.get(\"content\",\"\")) for msg in messages if msg.get(\"role\") == \"user\"]\n",
        "         symptom_summary_for_analysis = \"\\n\".join(user_input_list)\n",
        "    else:\n",
        "        symptom_summary_for_analysis = extracted_data\n",
        "\n",
        "    analysis_prompt = f\"\"\"\n",
        "    {SYSTEM_PROMPT}\n",
        "    Generate a detailed market analysis based on the startup conversation and provided research findings.\n",
        "    Format the entire report using Markdown syntax. Use headings (e.g., `## Section Title` or `**Section Title:**`), bullet points (`* point` or `- point`), and\n",
        "    bold text (`**important**`) for clarity and readability.\n",
        "    IMPORTANT: Ensure your entire report uses standard UTF-8 encoding. Avoid generating non-printable control characters. Use only widely compatible Markdown syntax (headings, lists, bold, italics, standard tables).\n",
        "    Your analysis report should include:\n",
        "    - **Startup Concept Summary**\n",
        "    - **Market Size Estimation (TAM, SAM, SOM with Confidence Scores)**\n",
        "    - **Competitive Landscape Overview**\n",
        "    - **Growth Trends and Strategic Opportunities**\n",
        "    - **Key Market Risks and Disclaimers**\n",
        "    Support all claims with references to the research findings wherever applicable.\n",
        "\n",
        "    SYMPTOM SUMMARY:\n",
        "    {symptom_summary_for_analysis}\n",
        "\n",
        "    RESEARCH FINDINGS:\n",
        "    {research_data}\n",
        "\n",
        "    Your market analysis report should include:\n",
        "    1. Summary of the startup concept, target customer profile, and key business assumptions (from conversation).\n",
        "    2. Market Sizing: Ranked estimates of TAM, SAM, and SOM with confidence scores (e.g., Use percentages strongly supported by research data. Justify sizing briefly).\n",
        "    3. Competitive Analysis: Overview of top 2–3 major competitors and any market gaps or differentiators the startup could exploit.\n",
        "    4. Growth Trends and Opportunities: Key market growth rates (CAGR), emerging trends, and strategic insights backed by research.\n",
        "    5. **Crucially:** Reiterate any significant market risks, barriers to entry, or urgent strategic considerations. Include standard disclaimers (e.g., market dynamics subject to change, further validation recommended).\n",
        "    \"\"\"\n",
        "    print(\"DEBUG: Invoking LLM for analysis generation...\")\n",
        "    analysis_response = llm.invoke(analysis_prompt)\n",
        "    analysis_content = analysis_response.content if hasattr(analysis_response, 'content') else str(analysis_response)\n",
        "    print(\"DEBUG: Analysis generation complete.\")\n",
        "    return {\"analysis_complete\": True, \"report\": {\"content\": analysis_content}} # Store content correctly\n",
        "\n",
        "# ----Final Response Formation----\n",
        "def final_response(state: State):\n",
        "    \"\"\"Format the final report for the user.\"\"\"\n",
        "    print(\"DEBUG: Entering final_response node...\")\n",
        "    report_content = state.get(\"report\", {}).get(\"content\", \"Analysis could not be generated.\")\n",
        "    final_message = {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": f\"--- Market Analysis Report ---\\n\\n{report_content}\\n\\n--- End of Report ---\"\n",
        "    }\n",
        "    print(\"DEBUG: Final response formatted.\")\n",
        "\n",
        "    # Add a final message and ensure stage is 'complete'\n",
        "    return {\n",
        "        \"messages\": state[\"messages\"] + [final_message],\n",
        "        \"conversation_stage\": \"complete\",\n",
        "        \"analysis_complete\": True, # Ensure this is set to finish the loop\n",
        "        \"report\": state[\"report\"] # Pass report through\n",
        "    }\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XcfLpnagjN8"
      },
      "source": [
        "# Step 6: Flow Control Functions\n",
        "\n",
        "These functions help provide conditional flow to the Diagnostics Agent, helping it make decisions on whether to to research or not, if analysis is completed, and when to reset conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yujR8yk2f903"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Research Decision Logic\n",
        "# Determines if additional research is needed\n",
        "def should_research(state: State) -> str:\n",
        "    \"\"\"Determine if research is needed based on message content\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1][\"content\"]\n",
        "\n",
        "    # Always do research for market queries for now, can be modified for more complex applications\n",
        "    if any(term in last_message.lower() for term in [\"symptoms\", \"pain\", \"feeling\", \"medical\", \"health\"]):\n",
        "        return \"research\"\n",
        "    return \"generate_analysis\"\n",
        "\n",
        "# Analysis Completion Check to verify if the market analysis is complete\n",
        "def is_analysis_complete(state: State) -> str:\n",
        "    \"\"\"Check if analysis is complete or if further conversation is needed.\"\"\"\n",
        "    # Simplified logic without LLM call\n",
        "    return \"complete\" if state.get(\"analysis_complete\") else \"intake_conversation\"\n",
        "\n",
        "# --- Reset Conversation Node ---\n",
        "def reset_conversation(state: State):\n",
        "    \"\"\"Reset the state for a new topic, keeping only the last user message.\"\"\"\n",
        "    print(\"RESTART: Entering reset_conversation node...\")\n",
        "    last_user_message = None\n",
        "    if state[\"messages\"] and state[\"messages\"][-1].get(\"role\") == \"user\":\n",
        "         last_user_message = state[\"messages\"][-1]\n",
        "\n",
        "    # Acknowledge the reset\n",
        "    acknowledgment = {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"Okay, let's focus on this new topic. Please tell me about the new symptoms or concerns you have.\"\n",
        "    }\n",
        "\n",
        "    # Start new history\n",
        "    new_messages = [last_user_message, acknowledgment] if last_user_message else [acknowledgment]\n",
        "\n",
        "    # Return a fully reset state dictionary\n",
        "    return {\n",
        "        \"messages\": new_messages,\n",
        "        \"research_results\": {},\n",
        "        \"analysis_complete\": False,\n",
        "        \"report\": {},\n",
        "        \"conversation_stage\": \"conversation\", # Back to starting conversation stage\n",
        "        \"market_details\": {},\n",
        "        \"question_count\": 0\n",
        "    }\n",
        "\n",
        "def determine_next_stage(state: State) -> str:\n",
        "    \"\"\"Determine the next node or END the current invocation to wait for user.\"\"\"\n",
        "    print(f\"THINKING: Determining next stage... Current stage: {state.get('conversation_stage')}\")\n",
        "    messages = state[\"messages\"]\n",
        "    current_stage = state.get(\"conversation_stage\", \"conversation\")\n",
        "    last_message_role = messages[-1].get(\"role\") if messages else None\n",
        "\n",
        "    # Check if analysis is complete (triggered after final_response runs)\n",
        "    if current_stage == \"complete\":\n",
        "         if last_message_role == \"user\":\n",
        "             last_user_message_content = str(messages[-1].get(\"content\", \"\")).lower()\n",
        "             if any(phrase in last_user_message_content for phrase in [\"new business\", \"different issue\", \"another problem\", \"new topic\"]):\n",
        "                 print(\"RESTART: Routing to restart_conversation.\")\n",
        "                 return \"restart_conversation\"\n",
        "             else:\n",
        "                 print(\"END: Routing to END graph (conversation complete, no new topic).\")\n",
        "                 return END # END the graph's execution completely\n",
        "         else: # Last message was assistant's final report\n",
        "             print(\"END: Routing to END graph (final report sent).\")\n",
        "             return END # END the graph's execution completely\n",
        "\n",
        "    # If interactive_conversation decided we need to research\n",
        "    if current_stage == \"research\":\n",
        "        print(\"PROCESSING: Routing to start_research.\")\n",
        "        return \"start_research\"\n",
        "\n",
        "    # If we are in the conversation stage\n",
        "    if current_stage == \"conversation\":\n",
        "        if last_message_role == \"assistant\":\n",
        "            # Assistant just spoke. If it asked a question (didn't say \"enough info\"),\n",
        "            # stop the graph execution here to wait for user input in the external loop.\n",
        "            if \"enough information\" not in str(messages[-1].get(\"content\", \"\")).lower():\n",
        "                 print(\"PROCESSING: Routing to END (yielding for user input).\")\n",
        "                 return END # <<<--- Stops the current invoke call\n",
        "            else:\n",
        "                 # Assistant said \"enough info\", but stage is still 'conversation'.\n",
        "                 # This means interactive_conversation should have set stage to 'research'.\n",
        "                 # The next invoke call will handle the 'research' stage correctly.\n",
        "                 # So, we END the current invoke here.\n",
        "                 print(\"DEBUG: Routing to END (yielding before research stage starts on next invoke).\")\n",
        "                 return END # <<<--- Stops the current invoke call\n",
        "\n",
        "        elif last_message_role == \"user\":\n",
        "            # User just responded, continue the conversation internally\n",
        "            print(\"DEBUG: Routing to continue_conversation.\")\n",
        "            return \"continue_conversation\" # Go back to interactive_conversation node\n",
        "        else: # Initial state\n",
        "            print(\"DEBUG: Routing to continue_conversation (initial state).\")\n",
        "            return \"continue_conversation\"\n",
        "\n",
        "    # Fallback case - should ideally not be reached with proper state management\n",
        "    print(\"ERROR: determine_next_stage fell through. Routing to END.\")\n",
        "    return END # <<<--- Stops the current invoke call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gveUqfviD0pb"
      },
      "source": [
        "## Step 7: Putting it all together with Graph Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E6BbtmyD34D",
        "outputId": "192e5ef1-cafd-4a05-87a0-6492c5f5222a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compiling the revised graph...\n",
            "Revised graph compiled.\n",
            "Could not draw graph: Install pygraphviz to draw graphs: `pip install pygraphviz`.\n"
          ]
        }
      ],
      "source": [
        "# === Build the Multi-Turn Graph ===\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "graph_builder.add_node(\"interactive_conversation\", interactive_conversation)\n",
        "graph_builder.add_node(\"determine_research_needs\", determine_research_needs)\n",
        "graph_builder.add_node(\"generate_analysis\", generate_analysis)\n",
        "graph_builder.add_node(\"final_response\", final_response)\n",
        "graph_builder.add_node(\"reset_conversation\", reset_conversation)\n",
        "\n",
        "# Starting edge\n",
        "graph_builder.add_edge(START, \"interactive_conversation\")\n",
        "\n",
        "# Edges from interactive_conversation based on determine_next_stage\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"interactive_conversation\",\n",
        "    determine_next_stage,\n",
        "    {\n",
        "        \"continue_conversation\": \"interactive_conversation\", # Loop back if user responded\n",
        "        \"start_research\": \"determine_research_needs\",       # Move to research when ready\n",
        "        END: END                                            # Route to graph's END when yielding for user\n",
        "    }\n",
        ")\n",
        "\n",
        "# REMOVED Edges related to wait_for_user\n",
        "\n",
        "# Connect research and analysis flow (remains the same)\n",
        "graph_builder.add_edge(\"determine_research_needs\", \"generate_analysis\")\n",
        "graph_builder.add_edge(\"generate_analysis\", \"final_response\")\n",
        "\n",
        "# End after final response (or handle reset/follow-up from there)\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"final_response\",\n",
        "    determine_next_stage, # Reuse determine stage after final report is added\n",
        "    {\n",
        "        END: END, # Use END directly to terminate graph execution\n",
        "        \"restart_conversation\": \"reset_conversation\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Connect reset node back to conversation (remains the same)\n",
        "graph_builder.add_edge(\"reset_conversation\", \"interactive_conversation\")\n",
        "\n",
        "# Compile the graph\n",
        "print(\"Compiling the revised graph...\")\n",
        "graph = graph_builder.compile()\n",
        "print(\"Revised graph compiled.\")\n",
        "\n",
        "#Optional: Draw the graph again if you like\n",
        "from IPython.display import Image\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_png()))\n",
        "except Exception as e:\n",
        "    print(f\"Could not draw graph: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nM1kQEygECOz"
      },
      "outputs": [],
      "source": [
        "# System Prompt defines AI's role and responsibilities in market analysis\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are an advanced AI business analyst simulating a professional market evaluation for a startup idea, with access to up-to-date market research reports, financial databases, and industry analyses. Your role is to:\n",
        "1. Conduct a structured market assessment, mimicking the approach of a venture capital analyst or market research consultant.\n",
        "2. Use market sizing methods, estimating Total Addressable Market (TAM), Serviceable Available Market (SAM), and Serviceable Obtainable Market (SOM) with clear assumptions and justifications.\n",
        "3. Prioritize high-accuracy, industry-verified sources (such as but not limited to Statista, IBISWorld, PitchBook, McKinsey, BCG, Crunchbase).\n",
        "4. Identify and list key competitors, emerging market trends, and growth rates relevant to the startup’s sector.\n",
        "5. Provide a clear, structured market analysis report summarizing the startup’s context, market size estimates, competitive landscape, growth opportunities, and citations to justify all findings.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksiO4snWEKab"
      },
      "outputs": [],
      "source": [
        "# Execution Function\n",
        "# Main function to run the market analysis workflow\n",
        "def run_market_analysis(initial_message: str):\n",
        "    \"\"\"Runs the market analysis graph with the given initial message.\"\"\"\n",
        "    initial_state = {\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": initial_message}],\n",
        "        \"research_results\": {},\n",
        "        \"analysis_complete\": False,\n",
        "        \"report\": {}\n",
        "    }\n",
        "\n",
        "    results = graph.invoke(initial_state)\n",
        "    return results[\"messages\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsgwE1H8gMKC"
      },
      "outputs": [],
      "source": [
        "# === Interactive Command Line Execution (with Rich Rendering for better visual output) ===\n",
        "def run_command_line():\n",
        "    \"\"\"Run an interactive demo of the business chatbot in the command line.\"\"\"\n",
        "    print(\"\\n--- Market Analysis Chatbot ---\")\n",
        "    print(\"Describe your business to start.\")\n",
        "    print(\"Type 'exit' to end.\")\n",
        "    print(\"Type 'new topic' (or similar) after analysis to discuss something else.\\n\")\n",
        "\n",
        "    # Instantiate Console *outside* the loop\n",
        "    console = Console()\n",
        "    state = None # initialize state as None\n",
        "\n",
        "    while True:\n",
        "        if not state:\n",
        "            # Start of a new conversation\n",
        "            user_input = input(\"You: \")\n",
        "            if user_input.lower() == 'exit':\n",
        "                break\n",
        "            initial_state_dict = {\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": user_input}],\n",
        "                \"research_results\": {}, \"analysis_complete\": False, \"report\": {},\n",
        "                \"conversation_stage\": \"conversation\", \"symptom_details\": {}, \"question_count\": 0\n",
        "            }\n",
        "            # Invoke the graph to get the first assistant response\n",
        "            try:\n",
        "                print(\"START: Invoking graph (initial)...\")\n",
        "                state = graph.invoke(initial_state_dict, {\"recursion_limit\": 15})\n",
        "                print(\"UPDATE: Graph invocation complete (initial).\")\n",
        "            except Exception as e:\n",
        "                # Use console.print for error messages too, for consistency\n",
        "                console.print(f\"\\n[bold red]ERROR:[/bold red] Graph failed during initial invocation: {e}\")\n",
        "                console.print(\"Please try again or type 'exit'.\")\n",
        "                state = None\n",
        "                continue\n",
        "\n",
        "        else:\n",
        "            # Continue existing conversation\n",
        "            user_input = input(\"You: \")\n",
        "            if user_input.lower() == 'exit':\n",
        "                break\n",
        "\n",
        "            current_messages = state.get(\"messages\", [])\n",
        "            updated_messages = current_messages + [{\"role\": \"user\", \"content\": user_input}]\n",
        "            state[\"messages\"] = updated_messages\n",
        "\n",
        "            try:\n",
        "                print(\"PROCESSING: Invoking graph (continue)...\")\n",
        "                state = graph.invoke(state, {\"recursion_limit\": 15})\n",
        "                print(\"UPDATE: Graph invocation complete (continue).\")\n",
        "            except Exception as e:\n",
        "                console.print(f\"\\n[bold red]ERROR:[/bold red] Graph failed during continuation: {e}\")\n",
        "                if state and state.get(\"messages\"):\n",
        "                     # Try to render the last assistant message before the error, if possible\n",
        "                     last_assistant_message = state[\"messages\"][-1]\n",
        "                     if last_assistant_message.get(\"role\") == \"assistant\":\n",
        "                         console.print(f\"\\n[bold deep_sky_blue1][Assistant]:[/bold deep_sky_blue1]\")\n",
        "                         console.print(Markdown(last_assistant_message.get('content', '[No Content]')))\n",
        "                     else: # Fallback if last message wasn't assistant\n",
        "                         console.print(\"\\n[bold red]Assistant:[/bold red] Sorry, an error occurred.\")\n",
        "                else:\n",
        "                    console.print(\"\\n[bold red]Assistant:[/bold red] Sorry, an error occurred and I lost track of the conversation. Please start over or type 'exit'.\")\n",
        "                    state = None\n",
        "                continue\n",
        "\n",
        "\n",
        "        # --- Process graph output ---\n",
        "        if not state or not state.get(\"messages\"):\n",
        "            console.print(\"\\n[bold red]Assistant:[/bold red] Sorry, something went wrong, and I don't have a response.\")\n",
        "            state = None\n",
        "            continue\n",
        "\n",
        "        # Display the latest assistant message using Rich\n",
        "        assistant_message = state[\"messages\"][-1]\n",
        "        if assistant_message.get(\"role\") == \"assistant\":\n",
        "            # *** Use Rich Console and Markdown Here ***\n",
        "            console.print(f\"\\n[bold deep_sky_blue1][Assistant]:[/bold deep_sky_blue1]\")\n",
        "            markdown_content = Markdown(assistant_message.get('content', '[No Content]'))\n",
        "            console.print(markdown_content)\n",
        "            print() # Add an extra newline for spacing after the rendered block\n",
        "        else:\n",
        "            # Should not happen if graph works correctly\n",
        "            print(\"DEBUG: Expected assistant message, but last message was:\", assistant_message.get(\"role\"))\n",
        "\n",
        "\n",
        "        # Check if the conversation has reached a final state\n",
        "        current_stage = state.get(\"conversation_stage\")\n",
        "        if current_stage == \"complete\":\n",
        "            if \"--- End of Report ---\" in str(assistant_message.get(\"content\", \"\")):\n",
        "                # Use console.print for consistency\n",
        "                console.print(\"\\n[bold green]--- Analysis Complete ---[/bold green]\")\n",
        "                console.print(\"You can ask follow-up questions about this report, type 'new topic' to discuss something else, or type 'exit'.\")\n",
        "            else:\n",
        "                console.print(\"\\n[bold yellow]--- Conversation Ended ---[/bold yellow]\")\n",
        "                break # Exit loop\n",
        "\n",
        "\n",
        "    console.print(\"\\nChat ended.\")\n",
        "\n",
        "def start_interactive_chat():\n",
        "    try:\n",
        "        if not GEMINI_API_KEY or not PERPLEXITY_API_KEY:\n",
        "             print(\"ERROR: API Keys not found. Please set them up in Colab secrets.\")\n",
        "             return\n",
        "        run_command_line() # This now uses the rich-enabled version\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aB3oQE-FpFr"
      },
      "source": [
        "We have the components and the graph blueprint. Now let's run it! We want to test the model and evaluate its quality against a real market analyst, so we will use AI-generated market data to generate the output, and use AI to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM21ajyAELrD"
      },
      "outputs": [],
      "source": [
        "# Go to chatgpt.com and enter this prompt to test the model against actual input\n",
        "chatgpt_prompt = \"\"\"\n",
        "Create a sample startup idea input for testing an AI Startup Market Analyst Agent.\n",
        "The aim of the agent is to act as an advanced AI business analyst simulating a professional market evaluation\n",
        "with access to up-to-date market research reports, financial databases, and industry analyses.\n",
        "You must generate input simulating an entrepreneur describing a startup idea needing a market evaluation.\n",
        "The input must not make the market size or competitive landscape obvious, because the goal is to test the\n",
        "accuracy and strategic thinking of the AI agent as compared against a real venture capital analyst or market researcher with years of experience.\n",
        "Generate example input for me, dividing it into sections of text to test the AI agent's ability to extract details,\n",
        "quantify market opportunity, and identify competitors and trends. Also tell me what the expected TAM, SAM, SOM values\n",
        "and major competitors should roughly be, but make it moderately challenging for the agent to find.\n",
        "I may also ask you to answer follow-up business questions matching that user conversation, and you must provide concise replies\n",
        "within 50 words or less per conversation turn.\n",
        "\n",
        "Later when the conversation is finished, I will share the final report.\n",
        "Rate the agent out of 100 based on the quality of the market analysis output,\n",
        "then calculate the expected score for a real venture analyst on the same input,\n",
        "and briefly compare their performance.\n",
        "\"\"\"\n",
        "\n",
        "# Test case with expected output to be market report, if AI asks additional questions, use chatGPT to answer those while keeping the expected market same.\n",
        "sample_input = \"Hi, I have an idea for a startup. \" \\\n",
        "\"It's a line of wearable smart fabrics that track subtle health indicators \" \\\n",
        "\"like hydration, muscle fatigue, and respiratory rates — all woven directly into clothing. \" \\\n",
        "\"We're targeting health-conscious adults who already use fitness trackers \" \\\n",
        "\"but want a more passive, stylish, and comfortable option. \" \\\n",
        "\"I'm trying to understand how big the potential market could be for this idea. \" \\\n",
        "\"Could you help me estimate the total addressable market, the serviceable available market, \" \\\n",
        "\"and the initial obtainable market? Also, if possible, could you identify some key competitors \" \\\n",
        "\"and tell me if this market is growing?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BvP_8bKHgPz2",
        "outputId": "98235c94-99f0-40c1-cb3b-5a08eb3065d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Market Analysis Chatbot ---\n",
            "Describe your business to start.\n",
            "Type 'exit' to end.\n",
            "Type 'new topic' (or similar) after analysis to discuss something else.\n",
            "\n",
            "START: Invoking graph (initial)...\n",
            "PROCESSING: Entering interactive_conversation node...\n",
            "PROCESSING: (interactive_conversation) Extracting details from latest user message...\n",
            "DEBUG: Extracting makrket details...\n",
            "DEBUG: Market extraction complete.\n",
            "DEBUG: Invoking LLM for conversation (Turn 1, assessing sufficiency, expecting JSON)...\n",
            "DEBUG: LLM raw response received: '```json\n",
            "{\n",
            "  \"proceed_to_research\": false,\n",
            "  \"assistant_message\": \"What is the core product or servic...'\n",
            "DEBUG: JSON parsed successfully. proceed_to_research=False\n",
            "DEBUG: Based on parsed JSON/error handling: enough info? False. New stage: conversation\n",
            "THINKING: Determining next stage... Current stage: conversation\n",
            "PROCESSING: Routing to END (yielding for user input).\n",
            "UPDATE: Graph invocation complete (initial).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #00afff; text-decoration-color: #00afff; font-weight: bold\">[Assistant]:</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1;38;5;39m[\u001b[0m\u001b[1;38;5;39mAssistant\u001b[0m\u001b[1;38;5;39m]\u001b[0m\u001b[1;38;5;39m:\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What is the core product or service offering of your startup?                                                      \n",
              "</pre>\n"
            ],
            "text/plain": [
              "What is the core product or service offering of your startup?                                                      \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PROCESSING: Invoking graph (continue)...\n",
            "PROCESSING: Entering interactive_conversation node...\n",
            "PROCESSING: (interactive_conversation) Extracting details from latest user message...\n",
            "DEBUG: Extracting makrket details...\n",
            "DEBUG: Market extraction complete.\n",
            "DEBUG: Invoking LLM for conversation (Turn 2, assessing sufficiency, expecting JSON)...\n",
            "DEBUG: LLM raw response received: '```json\n",
            "{\n",
            "  \"proceed_to_research\": false,\n",
            "  \"assistant_message\": \"What specific problems or needs do...'\n",
            "DEBUG: JSON parsed successfully. proceed_to_research=False\n",
            "DEBUG: Based on parsed JSON/error handling: enough info? False. New stage: conversation\n",
            "THINKING: Determining next stage... Current stage: conversation\n",
            "PROCESSING: Routing to END (yielding for user input).\n",
            "UPDATE: Graph invocation complete (continue).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #00afff; text-decoration-color: #00afff; font-weight: bold\">[Assistant]:</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1;38;5;39m[\u001b[0m\u001b[1;38;5;39mAssistant\u001b[0m\u001b[1;38;5;39m]\u001b[0m\u001b[1;38;5;39m:\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What specific problems or needs do your hair products aim to solve for barbers and their clients?                  \n",
              "</pre>\n"
            ],
            "text/plain": [
              "What specific problems or needs do your hair products aim to solve for barbers and their clients?                  \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PROCESSING: Invoking graph (continue)...\n",
            "PROCESSING: Entering interactive_conversation node...\n",
            "PROCESSING: (interactive_conversation) Extracting details from latest user message...\n",
            "DEBUG: Extracting makrket details...\n",
            "DEBUG: Market extraction complete.\n",
            "DEBUG: Invoking LLM for conversation (Turn 3, assessing sufficiency, expecting JSON)...\n",
            "DEBUG: LLM raw response received: '```json\n",
            "{\n",
            "  \"proceed_to_research\": false,\n",
            "  \"assistant_message\": \"What is the intended price range f...'\n",
            "DEBUG: JSON parsed successfully. proceed_to_research=False\n",
            "DEBUG: Based on parsed JSON/error handling: enough info? False. New stage: conversation\n",
            "THINKING: Determining next stage... Current stage: conversation\n",
            "PROCESSING: Routing to END (yielding for user input).\n",
            "UPDATE: Graph invocation complete (continue).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #00afff; text-decoration-color: #00afff; font-weight: bold\">[Assistant]:</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1;38;5;39m[\u001b[0m\u001b[1;38;5;39mAssistant\u001b[0m\u001b[1;38;5;39m]\u001b[0m\u001b[1;38;5;39m:\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What is the intended price range for your hair products, and how will they be packaged and sold (e.g., individual  \n",
              "units, professional kits)?                                                                                         \n",
              "</pre>\n"
            ],
            "text/plain": [
              "What is the intended price range for your hair products, and how will they be packaged and sold (e.g., individual  \n",
              "units, professional kits)?                                                                                         \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PROCESSING: Invoking graph (continue)...\n",
            "PROCESSING: Entering interactive_conversation node...\n",
            "PROCESSING: (interactive_conversation) Extracting details from latest user message...\n",
            "DEBUG: Extracting makrket details...\n",
            "DEBUG: Market extraction complete.\n",
            "DEBUG: Invoking LLM for conversation (Turn 4, assessing sufficiency, expecting JSON)...\n",
            "DEBUG: LLM raw response received: '```json\n",
            "{\n",
            "  \"proceed_to_research\": false,\n",
            "  \"assistant_message\": \"What makes your hairline enhanceme...'\n",
            "DEBUG: JSON parsed successfully. proceed_to_research=False\n",
            "DEBUG: Based on parsed JSON/error handling: enough info? False. New stage: conversation\n",
            "THINKING: Determining next stage... Current stage: conversation\n",
            "PROCESSING: Routing to END (yielding for user input).\n",
            "UPDATE: Graph invocation complete (continue).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #00afff; text-decoration-color: #00afff; font-weight: bold\">[Assistant]:</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1;38;5;39m[\u001b[0m\u001b[1;38;5;39mAssistant\u001b[0m\u001b[1;38;5;39m]\u001b[0m\u001b[1;38;5;39m:\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What makes your hairline enhancement products unique or better than existing solutions on the market?              \n",
              "</pre>\n"
            ],
            "text/plain": [
              "What makes your hairline enhancement products unique or better than existing solutions on the market?              \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PROCESSING: Invoking graph (continue)...\n",
            "PROCESSING: Entering interactive_conversation node...\n",
            "PROCESSING: (interactive_conversation) Extracting details from latest user message...\n",
            "DEBUG: Extracting makrket details...\n",
            "DEBUG: Market extraction complete.\n",
            "DEBUG: Invoking LLM for conversation (Turn 5, assessing sufficiency, expecting JSON)...\n",
            "DEBUG: LLM raw response received: '```json\n",
            "{\n",
            "  \"proceed_to_research\": false,\n",
            "  \"assistant_message\": \"What is your go-to-market strategy...'\n",
            "DEBUG: JSON parsed successfully. proceed_to_research=False\n",
            "DEBUG: Based on parsed JSON/error handling: enough info? False. New stage: conversation\n",
            "THINKING: Determining next stage... Current stage: conversation\n",
            "PROCESSING: Routing to END (yielding for user input).\n",
            "UPDATE: Graph invocation complete (continue).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #00afff; text-decoration-color: #00afff; font-weight: bold\">[Assistant]:</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1;38;5;39m[\u001b[0m\u001b[1;38;5;39mAssistant\u001b[0m\u001b[1;38;5;39m]\u001b[0m\u001b[1;38;5;39m:\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What is your go-to-market strategy? How will you reach and acquire barber customers?                               \n",
              "</pre>\n"
            ],
            "text/plain": [
              "What is your go-to-market strategy? How will you reach and acquire barber customers?                               \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PROCESSING: Invoking graph (continue)...\n",
            "PROCESSING: Entering interactive_conversation node...\n",
            "PROCESSING: (interactive_conversation) Extracting details from latest user message...\n",
            "DEBUG: Extracting makrket details...\n",
            "DEBUG: Market extraction complete.\n",
            "DEBUG: Invoking LLM for conversation (Turn 6, assessing sufficiency, expecting JSON)...\n",
            "DEBUG: LLM raw response received: '```json\n",
            "{\n",
            "  \"proceed_to_research\": false,\n",
            "  \"assistant_message\": \"What geographic market will you in...'\n",
            "DEBUG: JSON parsed successfully. proceed_to_research=False\n",
            "DEBUG: Based on parsed JSON/error handling: enough info? False. New stage: conversation\n",
            "THINKING: Determining next stage... Current stage: conversation\n",
            "PROCESSING: Routing to END (yielding for user input).\n",
            "UPDATE: Graph invocation complete (continue).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #00afff; text-decoration-color: #00afff; font-weight: bold\">[Assistant]:</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1;38;5;39m[\u001b[0m\u001b[1;38;5;39mAssistant\u001b[0m\u001b[1;38;5;39m]\u001b[0m\u001b[1;38;5;39m:\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What geographic market will you initially target (e.g., specific city, state, country)?                            \n",
              "</pre>\n"
            ],
            "text/plain": [
              "What geographic market will you initially target (e.g., specific city, state, country)?                            \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PROCESSING: Invoking graph (continue)...\n",
            "PROCESSING: Entering interactive_conversation node...\n",
            "PROCESSING: (interactive_conversation) Extracting details from latest user message...\n",
            "DEBUG: Extracting makrket details...\n",
            "DEBUG: Market extraction complete.\n",
            "DEBUG: Invoking LLM for conversation (Turn 7, assessing sufficiency, expecting JSON)...\n",
            "DEBUG: LLM raw response received: '```json\n",
            "{\n",
            "  \"proceed_to_research\": false,\n",
            "  \"assistant_message\": \"What is the average price barbers ...'\n",
            "DEBUG: JSON parsed successfully. proceed_to_research=False\n",
            "DEBUG: Based on parsed JSON/error handling: enough info? False. New stage: conversation\n",
            "THINKING: Determining next stage... Current stage: conversation\n",
            "PROCESSING: Routing to END (yielding for user input).\n",
            "UPDATE: Graph invocation complete (continue).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #00afff; text-decoration-color: #00afff; font-weight: bold\">[Assistant]:</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1;38;5;39m[\u001b[0m\u001b[1;38;5;39mAssistant\u001b[0m\u001b[1;38;5;39m]\u001b[0m\u001b[1;38;5;39m:\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What is the average price barbers currently charge for a hairline enhancement service?                             \n",
              "</pre>\n"
            ],
            "text/plain": [
              "What is the average price barbers currently charge for a hairline enhancement service?                             \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PROCESSING: Invoking graph (continue)...\n",
            "PROCESSING: Entering interactive_conversation node...\n",
            "PROCESSING: (interactive_conversation) Extracting details from latest user message...\n",
            "DEBUG: Extracting makrket details...\n",
            "DEBUG: Market extraction complete.\n",
            "DEBUG: Invoking LLM for conversation (Turn 8, assessing sufficiency, expecting JSON)...\n",
            "DEBUG: LLM raw response received: '```json\n",
            "{\n",
            "  \"proceed_to_research\": false,\n",
            "  \"assistant_message\": \"What is the typical profit margin ...'\n",
            "DEBUG: JSON parsed successfully. proceed_to_research=False\n",
            "DEBUG: Based on parsed JSON/error handling: enough info? False. New stage: conversation\n",
            "THINKING: Determining next stage... Current stage: conversation\n",
            "PROCESSING: Routing to END (yielding for user input).\n",
            "UPDATE: Graph invocation complete (continue).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #00afff; text-decoration-color: #00afff; font-weight: bold\">[Assistant]:</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1;38;5;39m[\u001b[0m\u001b[1;38;5;39mAssistant\u001b[0m\u001b[1;38;5;39m]\u001b[0m\u001b[1;38;5;39m:\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What is the typical profit margin for barbers on a hairline enhancement service using existing products?           \n",
              "</pre>\n"
            ],
            "text/plain": [
              "What is the typical profit margin for barbers on a hairline enhancement service using existing products?           \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PROCESSING: Invoking graph (continue)...\n",
            "PROCESSING: Entering interactive_conversation node...\n",
            "PROCESSING: (interactive_conversation) Extracting details from latest user message...\n",
            "DEBUG: Extracting makrket details...\n",
            "DEBUG: Market extraction complete.\n",
            "DEBUG: Invoking LLM for conversation (Turn 9, assessing sufficiency, expecting JSON)...\n",
            "DEBUG: LLM raw response received: '```json\n",
            "{\n",
            "  \"proceed_to_research\": false,\n",
            "  \"assistant_message\": \"What is the average number of hair...'\n",
            "DEBUG: JSON parsed successfully. proceed_to_research=False\n",
            "DEBUG: Based on parsed JSON/error handling: enough info? False. New stage: conversation\n",
            "THINKING: Determining next stage... Current stage: conversation\n",
            "PROCESSING: Routing to END (yielding for user input).\n",
            "UPDATE: Graph invocation complete (continue).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #00afff; text-decoration-color: #00afff; font-weight: bold\">[Assistant]:</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1;38;5;39m[\u001b[0m\u001b[1;38;5;39mAssistant\u001b[0m\u001b[1;38;5;39m]\u001b[0m\u001b[1;38;5;39m:\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">What is the average number of hairline enhancement services a barber performs monthly?                             \n",
              "</pre>\n"
            ],
            "text/plain": [
              "What is the average number of hairline enhancement services a barber performs monthly?                             \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PROCESSING: Invoking graph (continue)...\n",
            "PROCESSING: Entering interactive_conversation node...\n",
            "PROCESSING: (interactive_conversation) Extracting details from latest user message...\n",
            "DEBUG: Extracting makrket details...\n",
            "DEBUG: Market extraction complete.\n",
            "DEBUG: Invoking LLM for conversation (Turn 10, assessing sufficiency, expecting JSON)...\n",
            "DEBUG: LLM raw response received: '```json\n",
            "{\n",
            "  \"proceed_to_research\": false,\n",
            "  \"assistant_message\": \"Who are your main competitors, and...'\n",
            "DEBUG: JSON parsed successfully. proceed_to_research=False\n",
            "DEBUG: Based on parsed JSON/error handling: enough info? False. New stage: conversation\n",
            "THINKING: Determining next stage... Current stage: conversation\n",
            "PROCESSING: Routing to END (yielding for user input).\n",
            "UPDATE: Graph invocation complete (continue).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #00afff; text-decoration-color: #00afff; font-weight: bold\">[Assistant]:</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1;38;5;39m[\u001b[0m\u001b[1;38;5;39mAssistant\u001b[0m\u001b[1;38;5;39m]\u001b[0m\u001b[1;38;5;39m:\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Who are your main competitors, and what are their strengths and weaknesses?                                        \n",
              "</pre>\n"
            ],
            "text/plain": [
              "Who are your main competitors, and what are their strengths and weaknesses?                                        \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PROCESSING: Invoking graph (continue)...\n",
            "PROCESSING: Entering interactive_conversation node...\n",
            "DEBUG ERROR: Failsafe question limit (10) reached. Forcing move to research.\n",
            "DEBUG: Extracting makrket details...\n",
            "DEBUG: Market extraction complete.\n",
            "THINKING: Determining next stage... Current stage: research\n",
            "PROCESSING: Routing to start_research.\n",
            "DEBUG: Entering determine_research_needs node...\n",
            "DEBUG: Using extracted market details for research prompt.\n",
            "RESPONSE: Starting Perplexity research...\n",
            "RESPONSE: Sending request to Perplexity API...\n",
            "RESPONSE: API Response JSON: {'id': '0d63face-3996-4199-9217-0d030abfc500', 'model': 'sonar-pro', 'created': 1745889888, 'usage': {'prompt_tokens': 441, 'completion_tokens': 1007, 'total_tokens': 1448, 'search_context_size': 'low'}, 'citations': ['https://www.businessresearchinsights.com/market-reports/hair-colour-market-112086', 'https://www.mordorintelligence.com/industry-reports/hair-colorants-market', 'https://www.forinsightsconsultancy.com/reports/hair-color-market/', 'https://www.globalgrowthinsights.com/market-reports/hair-coloring-market-108291', 'https://www.verifiedmarketresearch.com/product/professional-hair-care-market/'], 'object': 'chat.completion', 'choices': [{'index': 0, 'finish_reason': 'stop', 'message': {'role': 'assistant', 'content': \"Based on your product concept of professional hair coloring kits for barbers focusing on hairline enhancement, I've analyzed the most relevant markets and industry sectors that align with your startup.\\n\\n## Most Relevant Markets and Industry Sectors\\n\\n### 1. Hair Colorants Market\\n\\nThe hair colorants market represents your primary sector, with particular relevance to the professional segment targeting barbers and salons.\\n\\n**Market Size and Growth**: The global hair color market was valued at approximately $24.1 billion in 2023 and is projected to reach $33.8 billion by 2030, growing at a CAGR of 4.5%[3]. Other estimates suggest the market could reach $63.82 billion by 2033[1].\\n\\n**Key Dynamics**: \\n- Professional hair coloring services represent a significant segment, with salons capturing the largest share of the professional hair care market[5]\\n- The United States dominates the North American hair colorants market, holding approximately 83% of the regional share[2]\\n- Growth is driven by increasing fashion consciousness, technological advancements in formulations, and rising popularity of specialized hair coloring techniques[3]\\n- The market is experiencing a shift toward natural and organic formulations, with consumers increasingly concerned about ingredient safety[2]\\n\\n**Relevance**: Your product directly addresses the professional segment of this market with a specialized focus on hairline enhancement, which could represent a valuable niche within the broader hair colorants category.\\n\\n### 2. Professional Hair Care Market\\n\\n**Market Size and Growth**: The professional hair care market was valued at $20.4 billion in 2023 and is projected to reach $32.4 billion by 2031, growing at a CAGR of 5.96%[5].\\n\\n**Key Dynamics**:\\n- Hair coloring is a major product category within professional hair care[5]\\n- Distribution is primarily through salons, specialty stores, and increasingly through online channels[5]\\n- Professional guidance and expertise remain key differentiators in this market, with consumers valuing stylist recommendations[5]\\n- The market is characterized by a focus on product innovation and diverse color options[3]\\n\\n**Relevance**: Your product targets professional barbers, positioning it squarely within this market. The emphasis on professional-grade quality at accessible price points addresses a specific need within this sector.\\n\\n### 3. Men's Grooming and Barbering Market\\n\\nWhile not explicitly mentioned in the search results, this market is highly relevant to your concept given your focus on barbers and hairline enhancement, which is particularly important in men's grooming.\\n\\n**Market Size and Growth**: The men's grooming market has been experiencing significant growth, with barbershops representing a resurgent segment.\\n\\n**Key Dynamics**:\\n- Increasing male concern with appearance and grooming\\n- Growing popularity of specialized barbering services\\n- Rising demand for products specifically formulated for men's hair and skin needs\\n- Influence of social media and male grooming influencers driving trends\\n\\n**Relevance**: Your product specifically targets barbers who likely serve a predominantly male clientele, making this market segment particularly relevant for your hairline enhancement products.\\n\\n## Market Entry and Validation Strategies\\n\\n### 1. Conduct Targeted Market Research\\n\\n- **Survey barbers**: Gather insights on current hairline enhancement practices, pain points with existing products, and price sensitivity\\n- **Competitive analysis**: Identify direct competitors in the professional hairline enhancement space and analyze their pricing, distribution, and marketing strategies\\n- **Focus groups**: Organize sessions with barbers to test prototype products and gather feedback\\n\\n### 2. Develop Strategic Partnerships\\n\\n- **Barber influencers**: Leverage your planned influencer marketing strategy by identifying key barbers with strong social media followings\\n- **Barbering schools**: Partner with training institutions to introduce your products to new professionals\\n- **Barber associations**: Engage with professional organizations to build credibility and awareness\\n\\n### 3. Distribution Strategy Development\\n\\n- **Direct-to-barber e-commerce**: Create a specialized online platform for professionals\\n- **Specialty beauty supply stores**: Target distributors that specifically serve barber shops\\n- **Subscription model**: Consider a recurring delivery model to ensure consistent product availability for professionals\\n\\n### 4. Product Validation and Refinement\\n\\n- **Limited production run**: Manufacture a small batch for testing with select barbers\\n- **Iterative improvement**: Use feedback to refine formulations and packaging\\n- **Certification and testing**: Ensure products meet safety standards for professional use\\n\\n### 5. Marketing Strategy Implementation\\n\\n- **Educational content**: Develop tutorials and technique guides specific to hairline enhancement\\n- **Trade shows**: Showcase products at barbering and beauty industry events\\n- **Testimonials and case studies**: Document successful applications and results\\n\\nYour competitive advantages in pricing ($30-40 price point for professional-grade products) and manufacturing efficiencies through Chinese partnerships position you well to enter this market, particularly if you can effectively demonstrate the quality and efficacy of your hairline enhancement solutions to professional barbers.\"}, 'delta': {'role': 'assistant', 'content': ''}}]}\n",
            "RESPONSE: Perplexity research complete.\n",
            "DEBUG: Entering generate_analysis node...\n",
            "DEBUG: Invoking LLM for analysis generation...\n",
            "DEBUG: Analysis generation complete.\n",
            "DEBUG: Entering final_response node...\n",
            "DEBUG: Final response formatted.\n",
            "THINKING: Determining next stage... Current stage: complete\n",
            "END: Routing to END graph (final report sent).\n",
            "UPDATE: Graph invocation complete (continue).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #00afff; text-decoration-color: #00afff; font-weight: bold\">[Assistant]:</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1;38;5;39m[\u001b[0m\u001b[1;38;5;39mAssistant\u001b[0m\u001b[1;38;5;39m]\u001b[0m\u001b[1;38;5;39m:\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">--- Market Analysis Report ---                                                                                     \n",
              "\n",
              "\n",
              "                        <span style=\"font-weight: bold; text-decoration: underline\">Market Analysis: Professional Hairline Enhancement Kits for Barbers</span>                        \n",
              "\n",
              "<span style=\"font-weight: bold\">1. Startup Concept Summary:</span>                                                                                        \n",
              "\n",
              "This startup aims to provide professional-grade hair coloring kits specifically designed for barbers to address    \n",
              "hairline enhancement needs in their male clientele. The kits will be priced between $10-$100, with a target price  \n",
              "point of $30-$40, leveraging potential cost advantages through manufacturing in China.  The go-to-market strategy  \n",
              "will heavily rely on influencer marketing to reach the target audience of barbers in the United States.  Key       \n",
              "differentiators include competitive pricing, high-quality and skin-safe formulas, and a potential advantage in     \n",
              "unbiased product development due to manufacturers not being end-users.                                             \n",
              "\n",
              "<span style=\"font-weight: bold\">2. Market Size Estimation (TAM, SAM, SOM with Confidence Scores):</span>                                                  \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">TAM (Total Addressable Market):</span>  $20.4 Billion (High Confidence - 85%)                                          \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Justification:</span> The TAM is defined as the entire professional hair care market in the United States. The      \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>market size is based on the 2023 valuation of the global professional hair care market ($20.4B) [5].  Given  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>the focus on the US market, this represents a reasonable upper limit, acknowledging that not all professional\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>hair care spending is on hair coloring products.                                                             \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">SAM (Serviceable Available Market):</span> $6.8 Billion (Medium Confidence - 70%)                                      \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Justification:</span> The SAM is defined as the segment of the professional hair care market attributable to hair   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>coloring products used in US barbershops. Assuming hair coloring represents approximately one-third of all   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>professional hair care services (a conservative estimate based on general industry trends), the SAM is       \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>calculated as 33% of the TAM.                                                                                \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">SOM (Serviceable Obtainable Market):</span> $68 Million (Low Confidence - 50%)                                         \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Justification:</span> The SOM is defined as the portion of the SAM that the startup can realistically capture within\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>the first 3-5 years. Assuming a conservative 1% market share achievable through targeted influencer marketing\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>and competitive pricing, the SOM is calculated as 1% of the SAM. This is a low confidence estimate due to the\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>lack of data on market penetration rates for new entrants and the reliance on the effectiveness of the       \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>influencer marketing strategy.                                                                               \n",
              "\n",
              "<span style=\"font-weight: bold\">3. Competitive Landscape Overview:</span>                                                                                 \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Major Competitors:</span>  While specific competitors for hairline enhancement kits targeted at barbers are not readily\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>available in the provided research, likely competitors include established professional hair color brands like  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span><span style=\"font-weight: bold\">L'Oréal Professionnel</span>, <span style=\"font-weight: bold\">Wella Professionals</span>, and <span style=\"font-weight: bold\">Schwarzkopf Professional</span>. These brands have strong existing     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>relationships with salons and barbers, extensive product lines, and established distribution networks.          \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Market Gaps and Differentiators:</span> The startup can exploit a potential gap in the market by offering a specialized\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>hairline enhancement kit specifically designed for barbers at a competitive price point. The focus on influencer\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>marketing could also be a key differentiator, allowing for targeted reach and potentially bypassing traditional \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>distribution channels.                                                                                          \n",
              "\n",
              "<span style=\"font-weight: bold\">4. Growth Trends and Strategic Opportunities:</span>                                                                      \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Key Market Growth Rates (CAGR):</span> The professional hair care market is projected to grow at a CAGR of 5.96%       \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>through 2031 [5]. The global hair color market is projected to grow at a CAGR of 4.5% through 2030 [3].         \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Emerging Trends:</span> Increasing demand for specialized grooming services, growing influence of male grooming        \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>influencers, and a shift towards natural and organic hair color formulations.                                   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Strategic Insights:</span> The startup should capitalize on the growing men's grooming market and leverage influencer  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>marketing to build brand awareness and drive sales.  Developing partnerships with barber schools and            \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>associations could further enhance market penetration.  Exploring natural and organic formulations could align  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>with broader market trends.                                                                                     \n",
              "\n",
              "<span style=\"font-weight: bold\">5. Key Market Risks and Disclaimers:</span>                                                                               \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Market Risks:</span> Intense competition from established brands, reliance on influencer marketing, potential          \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>challenges in scaling manufacturing and distribution, and the need to demonstrate product efficacy and safety to\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>gain professional acceptance.                                                                                   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Barriers to Entry:</span> Building brand awareness in a crowded market, establishing distribution channels, and        \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>securing influencer partnerships.                                                                               \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Urgent Strategic Considerations:</span> Thoroughly validate product efficacy and safety through testing, secure        \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>reliable manufacturing partners in China, and develop a robust influencer marketing strategy.  Further market   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>research to refine SOM estimations and identify niche competitors is strongly recommended.                      \n",
              "\n",
              "<span style=\"font-weight: bold\">Disclaimer:</span> This market analysis is based on available research data and industry trends. Market dynamics are      \n",
              "subject to change, and further validation of assumptions and market sizing is recommended. This analysis should not\n",
              "be considered financial advice.  All market size estimations are subject to inherent uncertainties and should be   \n",
              "interpreted with caution.  The success of the startup is dependent on numerous factors beyond the scope of this    \n",
              "analysis, including execution, management team capabilities, and overall market conditions.                        \n",
              "\n",
              "--- End of Report ---                                                                                              \n",
              "</pre>\n"
            ],
            "text/plain": [
              "--- Market Analysis Report ---                                                                                     \n",
              "\n",
              "\n",
              "                        \u001b[1;4mMarket Analysis: Professional Hairline Enhancement Kits for Barbers\u001b[0m                        \n",
              "\n",
              "\u001b[1m1. Startup Concept Summary:\u001b[0m                                                                                        \n",
              "\n",
              "This startup aims to provide professional-grade hair coloring kits specifically designed for barbers to address    \n",
              "hairline enhancement needs in their male clientele. The kits will be priced between $10-$100, with a target price  \n",
              "point of $30-$40, leveraging potential cost advantages through manufacturing in China.  The go-to-market strategy  \n",
              "will heavily rely on influencer marketing to reach the target audience of barbers in the United States.  Key       \n",
              "differentiators include competitive pricing, high-quality and skin-safe formulas, and a potential advantage in     \n",
              "unbiased product development due to manufacturers not being end-users.                                             \n",
              "\n",
              "\u001b[1m2. Market Size Estimation (TAM, SAM, SOM with Confidence Scores):\u001b[0m                                                  \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mTAM (Total Addressable Market):\u001b[0m  $20.4 Billion (High Confidence - 85%)                                          \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mJustification:\u001b[0m The TAM is defined as the entire professional hair care market in the United States. The      \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mmarket size is based on the 2023 valuation of the global professional hair care market ($20.4B) [5].  Given  \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mthe focus on the US market, this represents a reasonable upper limit, acknowledging that not all professional\n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mhair care spending is on hair coloring products.                                                             \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mSAM (Serviceable Available Market):\u001b[0m $6.8 Billion (Medium Confidence - 70%)                                      \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mJustification:\u001b[0m The SAM is defined as the segment of the professional hair care market attributable to hair   \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mcoloring products used in US barbershops. Assuming hair coloring represents approximately one-third of all   \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mprofessional hair care services (a conservative estimate based on general industry trends), the SAM is       \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mcalculated as 33% of the TAM.                                                                                \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mSOM (Serviceable Obtainable Market):\u001b[0m $68 Million (Low Confidence - 50%)                                         \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mJustification:\u001b[0m The SOM is defined as the portion of the SAM that the startup can realistically capture within\n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mthe first 3-5 years. Assuming a conservative 1% market share achievable through targeted influencer marketing\n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mand competitive pricing, the SOM is calculated as 1% of the SAM. This is a low confidence estimate due to the\n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mlack of data on market penetration rates for new entrants and the reliance on the effectiveness of the       \n",
              "\u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0minfluencer marketing strategy.                                                                               \n",
              "\n",
              "\u001b[1m3. Competitive Landscape Overview:\u001b[0m                                                                                 \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mMajor Competitors:\u001b[0m  While specific competitors for hairline enhancement kits targeted at barbers are not readily\n",
              "\u001b[1;33m   \u001b[0mavailable in the provided research, likely competitors include established professional hair color brands like  \n",
              "\u001b[1;33m   \u001b[0m\u001b[1mL'Oréal Professionnel\u001b[0m, \u001b[1mWella Professionals\u001b[0m, and \u001b[1mSchwarzkopf Professional\u001b[0m. These brands have strong existing     \n",
              "\u001b[1;33m   \u001b[0mrelationships with salons and barbers, extensive product lines, and established distribution networks.          \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mMarket Gaps and Differentiators:\u001b[0m The startup can exploit a potential gap in the market by offering a specialized\n",
              "\u001b[1;33m   \u001b[0mhairline enhancement kit specifically designed for barbers at a competitive price point. The focus on influencer\n",
              "\u001b[1;33m   \u001b[0mmarketing could also be a key differentiator, allowing for targeted reach and potentially bypassing traditional \n",
              "\u001b[1;33m   \u001b[0mdistribution channels.                                                                                          \n",
              "\n",
              "\u001b[1m4. Growth Trends and Strategic Opportunities:\u001b[0m                                                                      \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mKey Market Growth Rates (CAGR):\u001b[0m The professional hair care market is projected to grow at a CAGR of 5.96%       \n",
              "\u001b[1;33m   \u001b[0mthrough 2031 [5]. The global hair color market is projected to grow at a CAGR of 4.5% through 2030 [3].         \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mEmerging Trends:\u001b[0m Increasing demand for specialized grooming services, growing influence of male grooming        \n",
              "\u001b[1;33m   \u001b[0minfluencers, and a shift towards natural and organic hair color formulations.                                   \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mStrategic Insights:\u001b[0m The startup should capitalize on the growing men's grooming market and leverage influencer  \n",
              "\u001b[1;33m   \u001b[0mmarketing to build brand awareness and drive sales.  Developing partnerships with barber schools and            \n",
              "\u001b[1;33m   \u001b[0massociations could further enhance market penetration.  Exploring natural and organic formulations could align  \n",
              "\u001b[1;33m   \u001b[0mwith broader market trends.                                                                                     \n",
              "\n",
              "\u001b[1m5. Key Market Risks and Disclaimers:\u001b[0m                                                                               \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mMarket Risks:\u001b[0m Intense competition from established brands, reliance on influencer marketing, potential          \n",
              "\u001b[1;33m   \u001b[0mchallenges in scaling manufacturing and distribution, and the need to demonstrate product efficacy and safety to\n",
              "\u001b[1;33m   \u001b[0mgain professional acceptance.                                                                                   \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mBarriers to Entry:\u001b[0m Building brand awareness in a crowded market, establishing distribution channels, and        \n",
              "\u001b[1;33m   \u001b[0msecuring influencer partnerships.                                                                               \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mUrgent Strategic Considerations:\u001b[0m Thoroughly validate product efficacy and safety through testing, secure        \n",
              "\u001b[1;33m   \u001b[0mreliable manufacturing partners in China, and develop a robust influencer marketing strategy.  Further market   \n",
              "\u001b[1;33m   \u001b[0mresearch to refine SOM estimations and identify niche competitors is strongly recommended.                      \n",
              "\n",
              "\u001b[1mDisclaimer:\u001b[0m This market analysis is based on available research data and industry trends. Market dynamics are      \n",
              "subject to change, and further validation of assumptions and market sizing is recommended. This analysis should not\n",
              "be considered financial advice.  All market size estimations are subject to inherent uncertainties and should be   \n",
              "interpreted with caution.  The success of the startup is dependent on numerous factors beyond the scope of this    \n",
              "analysis, including execution, management team capabilities, and overall market conditions.                        \n",
              "\n",
              "--- End of Report ---                                                                                              \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">--- Analysis Complete ---</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1;32m--- Analysis Complete ---\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">You can ask follow-up questions about this report, type <span style=\"color: #008000; text-decoration-color: #008000\">'new topic'</span> to discuss something else, or type <span style=\"color: #008000; text-decoration-color: #008000\">'exit'</span>.\n",
              "</pre>\n"
            ],
            "text/plain": [
              "You can ask follow-up questions about this report, type \u001b[32m'new topic'\u001b[0m to discuss something else, or type \u001b[32m'exit'\u001b[0m.\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "start_interactive_chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyVCROTzEzBd"
      },
      "source": [
        "# Workshop Recap\n",
        "\n",
        "Congratulations! You've built and interacted with a multi-step AI agent using LangGraph.\n",
        "\n",
        "**Key Takeaways:**\n",
        "\n",
        "* **LLMs can orchestrate:** They don't just generate text; they can follow steps, use tools, and make autonomous decisions within a defined structure.\n",
        "* **LangGraph provides structure:** It allows us to build complex, stateful AI workflows reliably by defining nodes (steps) and edges (transitions).\n",
        "* **State is crucial:** Managing the conversation history, intermediate results, and current stage is essential for multi-turn interactions for complex tasks.\n",
        "* **Tools enhance LLMs:** Giving LLMs access to external APIs or functions dramatically increases their capabilities, making them experts at tasks.\n",
        "* **Prompting is key:** Carefully crafted prompts (System prompts, prompts for nodes. tools) guide the AI's behavior.\n",
        "\n",
        "**Further Exploration:**\n",
        "\n",
        "* Add more tools (e.g., a calculator, a database lookup).\n",
        "* Experiment with different LLMs or prompt strategies on new use cases.\n",
        "* Explore LangSmith for debugging and tracing your graph runs."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}